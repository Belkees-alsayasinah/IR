{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bfed57f-230a-4080-b102-95545bd1e7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.496078431372549, Average Recall: 0.496078431372549, Average MAP Score: 0.5621364379084968, Average MRR: 0.5913671023965141\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import logging\n",
    "import nltk\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n",
    "import unicodedata\n",
    "import contractions\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model for advanced text processing\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "with open(r\"C:\\Users\\sayas\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\common_words.txt\", 'r',\n",
    "          encoding='utf-8') as file:\n",
    "    words_to_remove = file.read().splitlines()\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    \n",
    "    def clean_text(self, text, words_to_remove):\n",
    "        words = text.split()\n",
    "        cleaned_words = [word for word in words if word not in words_to_remove]\n",
    "        cleaned_text = ' '.join(cleaned_words)\n",
    "        return cleaned_text\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted_words = []\n",
    "        for word in words:\n",
    "            # التحقق مما إذا كان النص يمثل رقمًا\n",
    "            if word.replace('.', '', 1).isdigit():  # إزالة النقطة العشرية قبل فحص الرقم\n",
    "                converted_words.append(word)\n",
    "            else:\n",
    "                if word.isdigit():\n",
    "                    try:\n",
    "                        num = int(word)\n",
    "                        if num <= 999999999999999:  # تحقق من طول الرقم\n",
    "                            converted_word = self.inflect_engine.number_to_words(word)\n",
    "                            converted_words.append(converted_word)\n",
    "                        else:\n",
    "                            converted_words.append(\"[Number Out of Range]\")\n",
    "                    except inflect.NumOutOfRangeError:\n",
    "                        converted_words.append(\"[Number Out of Range]\")\n",
    "                else:\n",
    "                    converted_words.append(word)\n",
    "        return ' '.join(converted_words)\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        try:\n",
    "            # Check if the input text contains HTML tags before parsing\n",
    "            if '<' in text and '>' in text:\n",
    "                return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "            else:\n",
    "                # If no HTML tags are found, return the original text\n",
    "                return text\n",
    "        except MarkupResemblesLocatorWarning:\n",
    "            # Handle the warning gracefully\n",
    "            logging.warning(\"MarkupResemblesLocatorWarning: The input looks more like a filename than markup.\")\n",
    "            # Return the original text if unable to parse as HTML\n",
    "            return text\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def remove_special_characters_and_emojis(self, text):\n",
    "        return re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "\n",
    "\n",
    "    def replace_synonyms(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        synonym_words = [self.get_synonym(word) for word in words]\n",
    "        return ' '.join(synonym_words)\n",
    "\n",
    "    def get_synonym(self, word):\n",
    "        synonyms = nltk.corpus.wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            return synonyms[0].lemmas()[0].name()\n",
    "        return word\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        negated_text = []\n",
    "        negate = False\n",
    "        for word in words:\n",
    "            if word.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                negated_text.append(f\"NOT_{word}\")\n",
    "                negate = False\n",
    "            else:\n",
    "                negated_text.append(word)\n",
    "        return ' '.join(negated_text)\n",
    "\n",
    "    def remove_non_english_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        english_words = [word for word in words if wordnet.synsets(word)]\n",
    "        return ' '.join(english_words)\n",
    "\n",
    "\n",
    "def process_text(text, processor):\n",
    "    if text is None:\n",
    "        return text\n",
    "    text = processor.cleaned_text(text)\n",
    "    text = processor.normalization_example(text)\n",
    "    text = processor.stemming_example(text)\n",
    "    text = processor.lemmatization_example(text)\n",
    "    text = processor.remove_stopwords(text)\n",
    "    text = processor.number_to_words(text)\n",
    "    text = processor.remove_punctuation(text)\n",
    "    text = processor.clean_text(text, words_to_remove)\n",
    "    text = processor.expand_contractions(text)\n",
    "    text = processor.normalize_unicode(text)\n",
    "    text = processor.handle_negations(text)\n",
    "    text = processor.remove_urls(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "sys.path.append('.')\n",
    "\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.5):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "\n",
    "def calculate_mrr(y_true):\n",
    "    rank_position = np.where(y_true == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)  # +1 because rank positions are 1-based\n",
    "\n",
    "def save_dataset(docs, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for pid, text in enumerate(docs, start=1):\n",
    "            file.write(f\"{pid}\\t{text}\\n\")\n",
    "\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, delimiter='\\t', header=None, names=['pid', 'text'])\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error reading the dataset file: {e}\")\n",
    "        sys.exit(1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "\n",
    "def process_texts(texts, processor):\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            processed_text = process_text(text, processor)\n",
    "            processed_texts.append(processed_text)\n",
    "        else:\n",
    "            print(\"Skipping non-string value:\", text)\n",
    "    return processed_texts\n",
    "\n",
    "\n",
    "def vectorize_texts(texts, processor):\n",
    "    vectorizer = TfidfVectorizer(preprocessor=lambda x: process_text(x, processor), max_df=0.5, min_df=1)\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during TF-IDF vectorization: {e}\")\n",
    "        print(f\"Sample texts: {texts[:5]}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "\n",
    "def get_documents_for_query(query, tfidf_matrix, processor, vectorizer, data):\n",
    "    processed_query = process_text(query, processor)\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix, query_vector).flatten()\n",
    "    n = 10\n",
    "    top_documents_indices = cosine_similarities.argsort()[-n:][::-1]\n",
    "    top_documents = data.iloc[top_documents_indices]\n",
    "    return top_documents, cosine_similarities[top_documents_indices]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    processor = TextProcessor()\n",
    "\n",
    "    dataset_path = r'C:\\Users\\sayas\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\collection.tsv'\n",
    "    data = load_dataset(dataset_path)\n",
    "    data.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "    if 'text' not in data.columns:\n",
    "        print(\"exit\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    tfidf_matrix, vectorizer = vectorize_texts(data['text'], processor)\n",
    "    queries_paths = [r'C:\\Users\\sayas\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\qas.search.jsonl']\n",
    "    queries = load_queries(queries_paths)\n",
    "\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_map_scores = []\n",
    "    all_mrrs = []\n",
    "\n",
    "    for query in queries:\n",
    "        if 'query' in query:\n",
    "\n",
    "            top_documents, cosine_similarities = get_documents_for_query(query['query'], tfidf_matrix, processor,\n",
    "                                                                         vectorizer, data)\n",
    "\n",
    "            relevance = np.zeros(len(data))\n",
    "            for pid in query.get('answer_pids', []):\n",
    "                relevance[np.where(data['pid'] == pid)[0]] = 1\n",
    "\n",
    "            relevantOrNot = relevance[top_documents.index]\n",
    "            retrievedDocument = cosine_similarities\n",
    "            if relevantOrNot.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            precision, recall = calculate_precision_recall(relevantOrNot, retrievedDocument)\n",
    "            all_precisions.append(precision)\n",
    "            all_recalls.append(recall)\n",
    "\n",
    "            map_score = calculate_map_score(relevantOrNot, retrievedDocument)\n",
    "            all_map_scores.append(map_score)\n",
    "\n",
    "            mrr = calculate_mrr(relevantOrNot)\n",
    "            all_mrrs.append(mrr)\n",
    "\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_map_score = np.mean(all_map_scores)\n",
    "    avg_mrr = np.mean(all_mrrs)\n",
    "\n",
    "    print(\n",
    "        f\"Average Precision: {avg_precision}, Average Recall: {avg_recall}, Average MAP Score: {avg_map_score}, Average MRR: {avg_mrr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41782bb2-460a-4154-a24c-d59a8ad96ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.6044444444444445, Average Recall: 0.6044444444444445, Average MAP Score: 0.7141095539038661, Average MRR: 0.8174029982363316\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import logging\n",
    "import nltk\n",
    "from nltk import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import inflect\n",
    "import re\n",
    "from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning\n",
    "import unicodedata\n",
    "import contractions\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model for advanced text processing\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "with open(r\"C:\\Users\\sayas\\.ir_datasets\\lotte\\lotte_extracted\\lotte\\lifestyle\\dev\\common_words.txt\", 'r',\n",
    "          encoding='utf-8') as file:\n",
    "    words_to_remove = file.read().splitlines()\n",
    "\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.inflect_engine = inflect.engine()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    \n",
    "    def clean_text(self, text, words_to_remove):\n",
    "        words = text.split()\n",
    "        cleaned_words = [word for word in words if word not in words_to_remove]\n",
    "        cleaned_text = ' '.join(cleaned_words)\n",
    "        return cleaned_text\n",
    "\n",
    "    def number_to_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        converted_words = []\n",
    "        for word in words:\n",
    "            # التحقق مما إذا كان النص يمثل رقمًا\n",
    "            if word.replace('.', '', 1).isdigit():  # إزالة النقطة العشرية قبل فحص الرقم\n",
    "                converted_words.append(word)\n",
    "            else:\n",
    "                if word.isdigit():\n",
    "                    try:\n",
    "                        num = int(word)\n",
    "                        if num <= 999999999999999:  # تحقق من طول الرقم\n",
    "                            converted_word = self.inflect_engine.number_to_words(word)\n",
    "                            converted_words.append(converted_word)\n",
    "                        else:\n",
    "                            converted_words.append(\"[Number Out of Range]\")\n",
    "                    except inflect.NumOutOfRangeError:\n",
    "                        converted_words.append(\"[Number Out of Range]\")\n",
    "                else:\n",
    "                    converted_words.append(word)\n",
    "        return ' '.join(converted_words)\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        try:\n",
    "            # Check if the input text contains HTML tags before parsing\n",
    "            if '<' in text and '>' in text:\n",
    "                return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "            else:\n",
    "                # If no HTML tags are found, return the original text\n",
    "                return text\n",
    "        except MarkupResemblesLocatorWarning:\n",
    "            # Handle the warning gracefully\n",
    "            logging.warning(\"MarkupResemblesLocatorWarning: The input looks more like a filename than markup.\")\n",
    "            # Return the original text if unable to parse as HTML\n",
    "            return text\n",
    "\n",
    "    def normalize_unicode(self, text):\n",
    "        return unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    def expand_contractions(self, text):\n",
    "        return contractions.fix(text)\n",
    "\n",
    "    def cleaned_text(self, text):\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "\n",
    "    def normalization_example(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def stemming_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        stemmed_words = [self.stemmer.stem(word) for word in words]\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    def lemmatization_example(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        lemmatized_words = [self.lemmatizer.lemmatize(word) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    def remove_urls(self, text):\n",
    "        return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    def remove_special_characters_and_emojis(self, text):\n",
    "        return re.sub(r'[^A-Za-z0-9\\s]+', '', text)\n",
    "\n",
    "\n",
    "    def replace_synonyms(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        synonym_words = [self.get_synonym(word) for word in words]\n",
    "        return ' '.join(synonym_words)\n",
    "\n",
    "    def get_synonym(self, word):\n",
    "        synonyms = nltk.corpus.wordnet.synsets(word)\n",
    "        if synonyms:\n",
    "            return synonyms[0].lemmas()[0].name()\n",
    "        return word\n",
    "\n",
    "    def handle_negations(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        negated_text = []\n",
    "        negate = False\n",
    "        for word in words:\n",
    "            if word.lower() in ['not', \"n't\"]:\n",
    "                negate = True\n",
    "            elif negate:\n",
    "                negated_text.append(f\"NOT_{word}\")\n",
    "                negate = False\n",
    "            else:\n",
    "                negated_text.append(word)\n",
    "        return ' '.join(negated_text)\n",
    "\n",
    "    def remove_non_english_words(self, text):\n",
    "        words = self.tokenizer.tokenize(text)\n",
    "        english_words = [word for word in words if wordnet.synsets(word)]\n",
    "        return ' '.join(english_words)\n",
    "\n",
    "\n",
    "def process_text(text, processor):\n",
    "    if text is None:\n",
    "        return text\n",
    "    text = processor.cleaned_text(text)\n",
    "    text = processor.normalization_example(text)\n",
    "    text = processor.stemming_example(text)\n",
    "    text = processor.lemmatization_example(text)\n",
    "    text = processor.remove_stopwords(text)\n",
    "    text = processor.number_to_words(text)\n",
    "    text = processor.remove_punctuation(text)\n",
    "    text = processor.clean_text(text, words_to_remove)\n",
    "    text = processor.expand_contractions(text)\n",
    "    text = processor.normalize_unicode(text)\n",
    "    text = processor.handle_negations(text)\n",
    "    text = processor.remove_urls(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "sys.path.append('.')\n",
    "\n",
    "\n",
    "def calculate_precision_recall(relevantOrNot, retrievedDocument, threshold=0.6):\n",
    "    binaryResult = (retrievedDocument >= threshold).astype(int)\n",
    "    precision = precision_score(relevantOrNot, binaryResult, average='micro')\n",
    "    recall = recall_score(relevantOrNot, binaryResult, average='micro')\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def calculate_map_score(relevantOrNot, retrievedDocument):\n",
    "    if np.sum(relevantOrNot) == 0:\n",
    "        return 0.0\n",
    "    return average_precision_score(relevantOrNot, retrievedDocument, average='micro')\n",
    "    \n",
    "def calculate_mrr(relevantOrNot):\n",
    "    rank_position = np.where(relevantOrNot == 1)[0]\n",
    "    if len(rank_position) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (rank_position[0] + 1)  # +1 because rank positions are 1-based\n",
    "\n",
    "def save_dataset(docs, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for pid, text in enumerate(docs, start=1):\n",
    "            file.write(f\"{pid}\\t{text}\\n\")\n",
    "\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, delimiter='\\t', header=None, names=['pid', 'text'])\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error reading the dataset file: {e}\")\n",
    "        sys.exit(1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_queries(queries_paths):\n",
    "    queries = []\n",
    "    for file_path in queries_paths:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                try:\n",
    "                    query = json.loads(line.strip())\n",
    "                    if 'query' in query:\n",
    "                        queries.append(query)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Skipping invalid line in {file_path}: {line}\")\n",
    "    return queries\n",
    "\n",
    "\n",
    "def process_texts(texts, processor):\n",
    "    processed_texts = []\n",
    "    for text in texts:\n",
    "        if isinstance(text, str):\n",
    "            processed_text = process_text(text, processor)\n",
    "            processed_texts.append(processed_text)\n",
    "        else:\n",
    "            print(\"Skipping non-string value:\", text)\n",
    "    return processed_texts\n",
    "\n",
    "\n",
    "def vectorize_texts(texts, processor):\n",
    "    vectorizer = TfidfVectorizer(preprocessor=lambda x: process_text(x, processor), max_df=0.5, min_df=1)\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during TF-IDF vectorization: {e}\")\n",
    "        print(f\"Sample texts: {texts[:5]}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    return tfidf_matrix, vectorizer\n",
    "    \n",
    "def save_tfidf_matrix_and_vectorizer(tfidf_matrix, vectorizer, matrix_file_path, vectorizer_file_path):\n",
    "    with open(matrix_file_path, 'wb') as file:\n",
    "        pickle.dump(tfidf_matrix, file)\n",
    "\n",
    "    with open(vectorizer_file_path, 'wb') as file:\n",
    "        pickle.dump(vectorizer, file)\n",
    "\n",
    "def get_documents_for_query(query, tfidf_matrix, processor, vectorizer, data):\n",
    "    processed_query = process_text(query, processor)\n",
    "    query_vector = vectorizer.transform([processed_query])\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix, query_vector).flatten()\n",
    "    n = 10\n",
    "    top_documents_indices = cosine_similarities.argsort()[-n:][::-1]\n",
    "    top_documents = data.iloc[top_documents_indices]\n",
    "    return top_documents, cosine_similarities[top_documents_indices]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    processor = TextProcessor()\n",
    "\n",
    "    dataset_path = r'C:\\Users\\sayas\\.ir_datasets\\antique\\train\\collection.tsv'\n",
    "    data = load_dataset(dataset_path)\n",
    "    data.dropna(subset=['text'], inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)  # Reset index here\n",
    "   \n",
    "   \n",
    "    if 'text' not in data.columns:\n",
    "        print(\"The dataset does not contain a 'text' column.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    tfidf_matrix, vectorizer = vectorize_texts(data['text'], processor)\n",
    "    queries_paths = [r'C:\\Users\\sayas\\.ir_datasets\\antique\\test\\Answers.jsonl']\n",
    "    queries = load_queries(queries_paths)\n",
    "\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_map_scores = []\n",
    "    all_mrrs = []\n",
    "\n",
    "    for query in queries:\n",
    "        if 'query' in query:\n",
    "            processed_query = process_text(query['query'], processor)\n",
    "\n",
    "            top_documents, cosine_similarities = get_documents_for_query(processed_query, tfidf_matrix, processor,\n",
    "                                                                         vectorizer, data)\n",
    "\n",
    "            relevance = np.zeros(len(data))\n",
    "            for pid in query.get('answer_pids', []):\n",
    "                relevance[np.where(data['pid'] == pid)[0]] = 1\n",
    "\n",
    "            relevantOrNot = relevance[top_documents.index]\n",
    "\n",
    "            retrievedDocument = cosine_similarities\n",
    "            if relevantOrNot.sum() == 0:\n",
    "                continue\n",
    "\n",
    "            precision, recall = calculate_precision_recall(relevantOrNot, retrievedDocument)\n",
    "            all_precisions.append(precision)\n",
    "            all_recalls.append(recall)\n",
    "\n",
    "            map_score = calculate_map_score(relevantOrNot, retrievedDocument)\n",
    "            all_map_scores.append(map_score)\n",
    "            \n",
    "            mrr = calculate_mrr(relevantOrNot)\n",
    "            all_mrrs.append(mrr)\n",
    "\n",
    "    avg_precision = np.mean(all_precisions)\n",
    "    avg_recall = np.mean(all_recalls)\n",
    "    avg_map_score = np.mean(all_map_scores)\n",
    "    avg_mrr = np.mean(all_mrrs)\n",
    "\n",
    "    print(f\"Average Precision: {avg_precision}, Average Recall: {avg_recall}, Average MAP Score: {avg_map_score}, Average MRR: {avg_mrr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144e8d5-931e-49e2-90ad-cf275b318526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
